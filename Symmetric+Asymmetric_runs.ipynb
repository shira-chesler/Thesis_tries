{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA6vP5rd8WWX",
        "outputId": "38bd1576-0a7c-4113-b9f8-b1ef5bfa28ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports zone\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from abc import abstractmethod\n",
        "from typing import Sequence, Union, Tuple\n",
        "\n",
        "# Scientific computing and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.modules.loss as loss\n",
        "\n",
        "# torchvision imports\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Google Colab imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "eTvA4m4zuRvf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 next cells are from [This github repository](https://github.com/AlexPasqua/Autoencoders/tree/main), changed to match the dataset we intend to work on currently (CIFAR100)."
      ],
      "metadata": {
        "id": "PuLTSqJ75qGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom_mnist but changed to be for CIFAR100\n",
        "'''\n",
        "# set device globally\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class FastCIFAR100(CIFAR100):\n",
        "    \"\"\"\n",
        "    The base of this class is taken from GitHub Gist, then I adapted it to my needs.\n",
        "    Author: Joost van Amersfoort (y0ast)\n",
        "    link: https://gist.github.com/y0ast/f69966e308e549f013a92dc66debeeb4\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Scale data to [0,1]\n",
        "        self.data = self.data.unsqueeze(1).float().div(255)\n",
        "\n",
        "        # Normalize it with the usual CIFAR100 mean and std\n",
        "        # self.data = self.data.sub_(0.1307).div_(0.3081)\n",
        "\n",
        "        # Since I'm working with autoencoders, 'targets' becomes a copy of the data. The labels are now stored\n",
        "        # in the variable 'labels'. Also put everything on GPU in advance, since the dataset is small.\n",
        "        # This lets me bypass PyTorch's DataLoaders and speed up the training.\n",
        "        self.data = self.data.to(device)\n",
        "        self.labels = self.targets.to(device)\n",
        "        self.targets = torch.flatten(copy.deepcopy(self.data), start_dim=1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target, label = self.data[index], self.targets[index], self.labels[index]\n",
        "        return img, target, label\n",
        "\n",
        "\n",
        "class NoisyCIFAR100(FastCIFAR100):\n",
        "    \"\"\" subclass of FastCIFAR100 with data=noisy_data and targets=clean_data (instead of labels) \"\"\"\n",
        "    def __init__(self, noise_const=0.1, patch_width=0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.data += noise_const * torch.randn(self.data.shape).to(device)\n",
        "        self.data -= torch.min(self.data)\n",
        "        self.data /= torch.max(self.data)\n",
        "        if patch_width > 0:\n",
        "            for img in self.data:\n",
        "                start = random.randint(0, img.shape[-1] - patch_width - 1)\n",
        "                img[:, start: start + patch_width, start: start + patch_width] = 0\n",
        "'''"
      ],
      "metadata": {
        "id": "E7R6Qrkf3pd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "9193bae5-a4cd-4610-9d1f-a397110d743d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# set device globally\\ndevice = torch.device(\\'cuda:0\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\nclass FastCIFAR100(CIFAR100):\\n    \"\"\"\\n    The base of this class is taken from GitHub Gist, then I adapted it to my needs.\\n    Author: Joost van Amersfoort (y0ast)\\n    link: https://gist.github.com/y0ast/f69966e308e549f013a92dc66debeeb4\\n    \"\"\"\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n        # Scale data to [0,1]\\n        self.data = self.data.unsqueeze(1).float().div(255)\\n\\n        # Normalize it with the usual CIFAR100 mean and std\\n        # self.data = self.data.sub_(0.1307).div_(0.3081)\\n\\n        # Since I\\'m working with autoencoders, \\'targets\\' becomes a copy of the data. The labels are now stored\\n        # in the variable \\'labels\\'. Also put everything on GPU in advance, since the dataset is small.\\n        # This lets me bypass PyTorch\\'s DataLoaders and speed up the training.\\n        self.data = self.data.to(device)\\n        self.labels = self.targets.to(device)\\n        self.targets = torch.flatten(copy.deepcopy(self.data), start_dim=1)\\n\\n    def __getitem__(self, index):\\n        \"\"\"\\n        Args:\\n            index (int): Index\\n\\n        Returns:\\n            tuple: (image, target) where target is index of the target class.\\n        \"\"\"\\n        img, target, label = self.data[index], self.targets[index], self.labels[index]\\n        return img, target, label\\n\\n\\nclass NoisyCIFAR100(FastCIFAR100):\\n    \"\"\" subclass of FastCIFAR100 with data=noisy_data and targets=clean_data (instead of labels) \"\"\"\\n    def __init__(self, noise_const=0.1, patch_width=0, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.data += noise_const * torch.randn(self.data.shape).to(device)\\n        self.data -= torch.min(self.data)\\n        self.data /= torch.max(self.data)\\n        if patch_width > 0:\\n            for img in self.data:\\n                start = random.randint(0, img.shape[-1] - patch_width - 1)\\n                img[:, start: start + patch_width, start: start + patch_width] = 0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom_losses\n",
        "class ContractiveLoss(loss.MSELoss):\n",
        "    \"\"\"\n",
        "    Custom loss for contractive autoencoders.\n",
        "\n",
        "    note: the superclass is MSELoss, simply because the base class _Loss is protected and it's not a best practice.\n",
        "          there isn't a real reason between the choice of MSELoss, since the forward method is overridden completely.\n",
        "\n",
        "    Overridden for elasticity -> it's possible to use a function as a custom loss, but having a wrapper class\n",
        "    allows to do:\n",
        "        criterion = ClassOfWhateverLoss()\n",
        "        loss = criterion(output, target)    # this line always the same regardless of the type on loss\n",
        "    \"\"\"\n",
        "    def __init__(self, ae, lambd: float, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
        "        super(ContractiveLoss, self).__init__(size_average, reduce, reduction)\n",
        "        self.ae = ae\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        return contractive_loss(input, target, self.lambd, self.ae, self.reduction)\n",
        "\n",
        "\n",
        "def contractive_loss(input, target, lambd, ae, reduction: str):\n",
        "    \"\"\"\n",
        "    Actual function computing the loss of a contractive autoencoder\n",
        "    :param input: (Tensor)\n",
        "    :param target: (Tensor)\n",
        "    :param lambd: (float) regularization parameter\n",
        "    :param ae: (DeepAutoencoder) the model itself, used to get it's weights\n",
        "    :param reduction: (str) type of reduction {'mean' | 'sum'}\n",
        "    :raises: ValueError\n",
        "    :return: the loss\n",
        "    \"\"\"\n",
        "    term1 = (input - target) ** 2\n",
        "    enc_weights = [ae.encoder[i].weight for i in reversed(range(1, len(ae.encoder), 2))]\n",
        "    term2 = lambd * torch.norm(torch.chain_matmul(*enc_weights))\n",
        "    contr_loss = torch.mean(term1 + term2, 0)\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(contr_loss)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(contr_loss)\n",
        "    else:\n",
        "        raise ValueError(f\"value for 'reduction' must be 'mean' or 'sum', got {reduction}\")"
      ],
      "metadata": {
        "id": "1py0TP4n3IcM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_utilities\n",
        "'''\n",
        "# set device globally\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# these variables will be allocated only if needed\n",
        "CIFAR100_train = None\n",
        "CIFAR100_test = None\n",
        "noisy_CIFAR100_train = None\n",
        "noisy_CIFAR100_test = None\n",
        "\n",
        "\n",
        "def get_clean_sets():\n",
        "    global CIFAR100_train\n",
        "    global CIFAR100_test\n",
        "    if CIFAR100_train is None:\n",
        "        CIFAR100_train = CIFAR100(root='../CIFAR100/', train=True, download=True, transform=transforms.ToTensor())\n",
        "        CIFAR100_test = CIFAR100(root='../CIFAR100/', train=False, download=True, transform=transforms.ToTensor())\n",
        "    return CIFAR100_train, CIFAR100_test\n",
        "\n",
        "\n",
        "def get_noisy_sets(**kwargs):\n",
        "    global noisy_CIFAR100_train\n",
        "    global noisy_CIFAR100_test\n",
        "    if noisy_CIFAR100_train is None:\n",
        "        noisy_CIFAR100_train = NoisyCIFAR100(root='../CIFAR100/', train=True, download=True, transform=transforms.ToTensor(), **kwargs)\n",
        "        noisy_CIFAR100_test = NoisyCIFAR100(root='../CIFAR100/', train=False, download=True, transform=transforms.ToTensor(), **kwargs)\n",
        "    return noisy_CIFAR100_train, noisy_CIFAR100_test\n",
        "\n",
        "\n",
        "def fit_ae(model, mode=None, tr_data=None, val_data=None, num_epochs=10, bs=32, lr=0.1, momentum=0., **kwargs):\n",
        "    \"\"\"\n",
        "    Training functions for the AEs\n",
        "    :param model: model to train\n",
        "    :param mode: (str) {'basic | 'contractive' | 'denoising'}\n",
        "    :param tr_data: (optional) specific training data to use\n",
        "    :param val_data: (optional) specific validation data to use\n",
        "    :param num_epochs: (int) number of epochs\n",
        "    :param bs: (int) batch size\n",
        "    :param lr: (float) learning rate\n",
        "    :param momentum: (float) momentum coefficient\n",
        "    :return: history of training (like in Keras)\n",
        "    \"\"\"\n",
        "    mode_values = (None, 'basic', 'contractive', 'denoising')\n",
        "    assert 0 < lr < 1 and num_epochs > 0 and bs > 0 and 0 <= momentum < 1 and mode in mode_values\n",
        "\n",
        "    # set the device: GPU if cuda is available, else CPU\n",
        "    model.to(device)\n",
        "\n",
        "    # set optimizer, loss type and datasets (depending on the type of AE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    criterion = ContractiveLoss(ae=model, lambd=1e-4) if mode == 'contractive' else nn.MSELoss()\n",
        "    if mode == 'denoising':\n",
        "        if tr_data is not None or val_data is not None:\n",
        "            warnings.warn(\"'denoising' flag was set, so NoisyCIFAR100 will be used for training and validation\")\n",
        "        noisy_train, noisy_val = get_noisy_sets(**kwargs)\n",
        "        tr_data, tr_targets = noisy_train.data, noisy_train.targets\n",
        "        val_data, val_targets = noisy_val.data, noisy_val.targets\n",
        "        del noisy_train, noisy_val\n",
        "    else:\n",
        "        tr_set, val_set = get_clean_sets()\n",
        "        if tr_data is None:\n",
        "            tr_data, tr_targets = tr_set.data, tr_set.targets\n",
        "        else:\n",
        "            tr_data = tr_data.to(device)\n",
        "            tr_targets = torch.flatten(copy.deepcopy(tr_data), start_dim=1)\n",
        "        if val_data is None:\n",
        "            val_data, val_targets = val_set.data, val_set.targets\n",
        "        else:\n",
        "            val_data = val_data.to(device)\n",
        "            val_targets = torch.flatten(copy.deepcopy(val_data), start_dim=1)\n",
        "        del tr_set, val_set\n",
        "    if 'ConvAutoencoder' in model.__class__.__name__:\n",
        "        val_bs = bs\n",
        "        tr_data, tr_targets = tr_data.cpu(), tr_targets.cpu()\n",
        "        val_data, val_targets = val_data.cpu(), val_targets.cpu()\n",
        "    else:\n",
        "        val_bs = None\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # training cycle\n",
        "    loss = None  # just to avoid reference before assigment\n",
        "    history = {'tr_loss': [], 'val_loss': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        # training\n",
        "        model.train()\n",
        "        tr_loss = 0\n",
        "        n_batches = math.ceil(len(tr_data) / bs)\n",
        "        # shuffle\n",
        "        indexes = torch.randperm(tr_data.shape[0])\n",
        "        tr_data = tr_data[indexes]\n",
        "        tr_targets = tr_targets[indexes]\n",
        "        progbar = tqdm(range(n_batches), total=n_batches)\n",
        "        progbar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        for batch_idx in range(n_batches):\n",
        "            # zero the gradient\n",
        "            optimizer.zero_grad()\n",
        "            # select a (mini)batch from the training set and compute net's outputs\n",
        "            train_data_batch = tr_data[batch_idx * bs: batch_idx * bs + bs].to(device)\n",
        "            train_targets_batch = tr_targets[batch_idx * bs: batch_idx * bs + bs].to(device)\n",
        "            outputs = model(train_data_batch)\n",
        "            # compute loss (flatten output in case of ConvAE. targets already flat)\n",
        "            loss = criterion(torch.flatten(outputs, 1), train_targets_batch)\n",
        "            tr_loss += loss.item()\n",
        "            # propagate back the loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # update progress bar\n",
        "            progbar.update()\n",
        "            progbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
        "        last_batch_loss = loss.item()\n",
        "        tr_loss /= n_batches\n",
        "        history['tr_loss'].append(round(tr_loss, 5))\n",
        "\n",
        "        # validation\n",
        "        val_loss = evaluate(model=model, data=val_data, targets=val_targets, criterion=criterion, bs=val_bs)\n",
        "        history['val_loss'].append(round(val_loss, 5))\n",
        "        torch.cuda.empty_cache()\n",
        "        progbar.set_postfix(train_loss=f\"{last_batch_loss:.4f}\", val_loss=f\"{val_loss:.4f}\")\n",
        "        progbar.close()\n",
        "\n",
        "        # simple early stopping mechanism\n",
        "        if epoch >= 10:\n",
        "            last_values = history['val_loss'][-10:]\n",
        "            if (abs(last_values[-10] - last_values[-1]) <= 2e-5) or (last_values[-3] < last_values[-2] < last_values[-1]):\n",
        "                return history\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, mode='basic', data=None, targets=None, bs=None, **kwargs):\n",
        "    \"\"\" Evaluate the model \"\"\"\n",
        "    # set the data\n",
        "    if data is None:\n",
        "        _, val_set = get_noisy_sets(**kwargs) if mode == 'denoising' else get_clean_sets()\n",
        "        data, targets = val_set.data, val_set.targets\n",
        "    bs = len(data) if bs is None else bs\n",
        "    n_batches = math.ceil(len(data) / bs)\n",
        "    if 'ConvAutoencoder' in model.__class__.__name__:\n",
        "        data = data.to('cpu')\n",
        "        targets = targets.to('cpu')\n",
        "    else:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "    # evaluate\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_idx in range(n_batches):\n",
        "            data_batch = data[batch_idx * bs: batch_idx * bs + bs].to(device)\n",
        "            targets_batch = targets[batch_idx * bs: batch_idx * bs + bs].to(device)\n",
        "            outputs = model(data_batch)\n",
        "            # flatten outputs in case of ConvAE (targets already flat)\n",
        "            loss = criterion(torch.flatten(outputs, 1), targets_batch)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / n_batches\n",
        "'''"
      ],
      "metadata": {
        "id": "2bOBd7Va29R2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "c1d0c3b0-8980-4e36-ab69-91018e2b0e37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# set device globally\\ndevice = torch.device(\\'cuda:0\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n# these variables will be allocated only if needed\\nCIFAR100_train = None\\nCIFAR100_test = None\\nnoisy_CIFAR100_train = None\\nnoisy_CIFAR100_test = None\\n\\n\\ndef get_clean_sets():\\n    global CIFAR100_train\\n    global CIFAR100_test\\n    if CIFAR100_train is None:\\n        CIFAR100_train = CIFAR100(root=\\'../CIFAR100/\\', train=True, download=True, transform=transforms.ToTensor())\\n        CIFAR100_test = CIFAR100(root=\\'../CIFAR100/\\', train=False, download=True, transform=transforms.ToTensor())\\n    return CIFAR100_train, CIFAR100_test\\n\\n\\ndef get_noisy_sets(**kwargs):\\n    global noisy_CIFAR100_train\\n    global noisy_CIFAR100_test\\n    if noisy_CIFAR100_train is None:\\n        noisy_CIFAR100_train = NoisyCIFAR100(root=\\'../CIFAR100/\\', train=True, download=True, transform=transforms.ToTensor(), **kwargs)\\n        noisy_CIFAR100_test = NoisyCIFAR100(root=\\'../CIFAR100/\\', train=False, download=True, transform=transforms.ToTensor(), **kwargs)\\n    return noisy_CIFAR100_train, noisy_CIFAR100_test\\n\\n\\ndef fit_ae(model, mode=None, tr_data=None, val_data=None, num_epochs=10, bs=32, lr=0.1, momentum=0., **kwargs):\\n    \"\"\"\\n    Training functions for the AEs\\n    :param model: model to train\\n    :param mode: (str) {\\'basic | \\'contractive\\' | \\'denoising\\'}\\n    :param tr_data: (optional) specific training data to use\\n    :param val_data: (optional) specific validation data to use\\n    :param num_epochs: (int) number of epochs\\n    :param bs: (int) batch size\\n    :param lr: (float) learning rate\\n    :param momentum: (float) momentum coefficient\\n    :return: history of training (like in Keras)\\n    \"\"\"\\n    mode_values = (None, \\'basic\\', \\'contractive\\', \\'denoising\\')\\n    assert 0 < lr < 1 and num_epochs > 0 and bs > 0 and 0 <= momentum < 1 and mode in mode_values\\n\\n    # set the device: GPU if cuda is available, else CPU\\n    model.to(device)\\n\\n    # set optimizer, loss type and datasets (depending on the type of AE)\\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\\n    criterion = ContractiveLoss(ae=model, lambd=1e-4) if mode == \\'contractive\\' else nn.MSELoss()\\n    if mode == \\'denoising\\':\\n        if tr_data is not None or val_data is not None:\\n            warnings.warn(\"\\'denoising\\' flag was set, so NoisyCIFAR100 will be used for training and validation\")\\n        noisy_train, noisy_val = get_noisy_sets(**kwargs)\\n        tr_data, tr_targets = noisy_train.data, noisy_train.targets\\n        val_data, val_targets = noisy_val.data, noisy_val.targets\\n        del noisy_train, noisy_val\\n    else:\\n        tr_set, val_set = get_clean_sets()\\n        if tr_data is None:\\n            tr_data, tr_targets = tr_set.data, tr_set.targets\\n        else:\\n            tr_data = tr_data.to(device)\\n            tr_targets = torch.flatten(copy.deepcopy(tr_data), start_dim=1)\\n        if val_data is None:\\n            val_data, val_targets = val_set.data, val_set.targets\\n        else:\\n            val_data = val_data.to(device)\\n            val_targets = torch.flatten(copy.deepcopy(val_data), start_dim=1)\\n        del tr_set, val_set\\n    if \\'ConvAutoencoder\\' in model.__class__.__name__:\\n        val_bs = bs\\n        tr_data, tr_targets = tr_data.cpu(), tr_targets.cpu()\\n        val_data, val_targets = val_data.cpu(), val_targets.cpu()\\n    else:\\n        val_bs = None\\n    torch.cuda.empty_cache()\\n\\n    # training cycle\\n    loss = None  # just to avoid reference before assigment\\n    history = {\\'tr_loss\\': [], \\'val_loss\\': []}\\n    for epoch in range(num_epochs):\\n        # training\\n        model.train()\\n        tr_loss = 0\\n        n_batches = math.ceil(len(tr_data) / bs)\\n        # shuffle\\n        indexes = torch.randperm(tr_data.shape[0])\\n        tr_data = tr_data[indexes]\\n        tr_targets = tr_targets[indexes]\\n        progbar = tqdm(range(n_batches), total=n_batches)\\n        progbar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\\n        for batch_idx in range(n_batches):\\n            # zero the gradient\\n            optimizer.zero_grad()\\n            # select a (mini)batch from the training set and compute net\\'s outputs\\n            train_data_batch = tr_data[batch_idx * bs: batch_idx * bs + bs].to(device)\\n            train_targets_batch = tr_targets[batch_idx * bs: batch_idx * bs + bs].to(device)\\n            outputs = model(train_data_batch)\\n            # compute loss (flatten output in case of ConvAE. targets already flat)\\n            loss = criterion(torch.flatten(outputs, 1), train_targets_batch)\\n            tr_loss += loss.item()\\n            # propagate back the loss\\n            loss.backward()\\n            optimizer.step()\\n            # update progress bar\\n            progbar.update()\\n            progbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\\n        last_batch_loss = loss.item()\\n        tr_loss /= n_batches\\n        history[\\'tr_loss\\'].append(round(tr_loss, 5))\\n\\n        # validation\\n        val_loss = evaluate(model=model, data=val_data, targets=val_targets, criterion=criterion, bs=val_bs)\\n        history[\\'val_loss\\'].append(round(val_loss, 5))\\n        torch.cuda.empty_cache()\\n        progbar.set_postfix(train_loss=f\"{last_batch_loss:.4f}\", val_loss=f\"{val_loss:.4f}\")\\n        progbar.close()\\n\\n        # simple early stopping mechanism\\n        if epoch >= 10:\\n            last_values = history[\\'val_loss\\'][-10:]\\n            if (abs(last_values[-10] - last_values[-1]) <= 2e-5) or (last_values[-3] < last_values[-2] < last_values[-1]):\\n                return history\\n\\n    return history\\n\\n\\ndef evaluate(model, criterion, mode=\\'basic\\', data=None, targets=None, bs=None, **kwargs):\\n    \"\"\" Evaluate the model \"\"\"\\n    # set the data\\n    if data is None:\\n        _, val_set = get_noisy_sets(**kwargs) if mode == \\'denoising\\' else get_clean_sets()\\n        data, targets = val_set.data, val_set.targets\\n    bs = len(data) if bs is None else bs\\n    n_batches = math.ceil(len(data) / bs)\\n    if \\'ConvAutoencoder\\' in model.__class__.__name__:\\n        data = data.to(\\'cpu\\')\\n        targets = targets.to(\\'cpu\\')\\n    else:\\n        data = data.to(device)\\n        targets = targets.to(device)\\n\\n    # evaluate\\n    model.to(device)\\n    model.eval()\\n    with torch.no_grad():\\n        val_loss = 0\\n        for batch_idx in range(n_batches):\\n            data_batch = data[batch_idx * bs: batch_idx * bs + bs].to(device)\\n            targets_batch = targets[batch_idx * bs: batch_idx * bs + bs].to(device)\\n            outputs = model(data_batch)\\n            # flatten outputs in case of ConvAE (targets already flat)\\n            loss = criterion(torch.flatten(outputs, 1), targets_batch)\\n            val_loss += loss.item()\\n    return val_loss / n_batches\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# symmetric autoencoder\n",
        "\n",
        "# set device globally\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class AbstractAutoencoder(nn.Module):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    '''def fit(self, mode='basic', tr_data=None, val_data=None, num_epochs=10, bs=32, lr=0.1, momentum=0., **kwargs):\n",
        "        return fit_ae(model=self, mode=mode, tr_data=tr_data, val_data=val_data, num_epochs=num_epochs, bs=bs, lr=lr,\n",
        "                      momentum=momentum, **kwargs)'''\n",
        "\n",
        "    def show_manifold_convergence(self, load=None, path=None, max_iters=1000, thresh=0.02, side_len=28, save=False):\n",
        "        \"\"\"\n",
        "        Show the manifold convergence of an AE when fed with random noise.\n",
        "        The output of the AE is fed again as input in an iterative process.\n",
        "        :param load: if True, load an images progression of the manifold convergence\n",
        "        :param path: path of the images progression\n",
        "        :param max_iters: max number of iterations.\n",
        "        :param thresh: threshold of MSE between 2 iterations under which the process is stopped\n",
        "        :param side_len: length of the side of the images\n",
        "        :param save: if True, save the images progression and the animation\n",
        "        \"\"\"\n",
        "        if load:\n",
        "            images_progression = np.load(path)\n",
        "        else:\n",
        "            self.cpu()\n",
        "            noise_img = torch.randn((1, 1, side_len, side_len))\n",
        "            noise_img -= torch.min(noise_img)\n",
        "            noise_img /= torch.max(noise_img)\n",
        "            images_progression = [torch.squeeze(noise_img)]\n",
        "            serializable_progression = [torch.squeeze(noise_img).cpu().numpy()]\n",
        "\n",
        "            # iterate\n",
        "            i = 0\n",
        "            loss = 1000\n",
        "            input = noise_img\n",
        "            prev_output = None\n",
        "            with torch.no_grad():\n",
        "                while loss > thresh and i < max_iters:\n",
        "                    output = self(input)\n",
        "                    img = torch.reshape(torch.squeeze(output), shape=(side_len, side_len))\n",
        "                    rescaled_img = (img - torch.min(img)) / torch.max(img)\n",
        "                    images_progression.append(rescaled_img)\n",
        "                    serializable_progression.append(rescaled_img.cpu().numpy())\n",
        "                    if prev_output is not None:\n",
        "                        loss = F.mse_loss(output, prev_output)\n",
        "                    prev_output = output\n",
        "                    input = output\n",
        "                    i += 1\n",
        "\n",
        "            # save sequence of images\n",
        "            if save:\n",
        "                serializable_progression = np.array(serializable_progression)\n",
        "                np.save(file=\"manifold_img_seq\", arr=serializable_progression)\n",
        "\n",
        "        if save:\n",
        "            images_progression = images_progression[:60]\n",
        "            frames = []  # for storing the generated images\n",
        "            fig = plt.figure()\n",
        "            for i in range(len(images_progression)):\n",
        "                frames.append([plt.imshow(images_progression[i], animated=True)])\n",
        "            ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True, repeat_delay=1000)\n",
        "            ani.save('movie.gif')\n",
        "            plt.show()\n",
        "        else:\n",
        "            # show images progression\n",
        "            img = None\n",
        "            for i in range(len(images_progression)):\n",
        "                if img is None:\n",
        "                    img = plt.imshow(images_progression[0])\n",
        "                else:\n",
        "                    img.set_data(images_progression[i])\n",
        "                plt.pause(.1)\n",
        "                plt.draw()\n",
        "\n",
        "'''\n",
        "class ShallowAutoencoder(AbstractAutoencoder):\n",
        "    \"\"\" Standard shallow AE with 1 fully-connected layer in the encoder and 1 in the decoder \"\"\"\n",
        "    def __init__(self, input_dim: int = 784, latent_dim: int = 200, use_bias=True):\n",
        "        super().__init__()\n",
        "        assert input_dim > 0 and latent_dim > 0\n",
        "        self.type = \"shallowAE\"\n",
        "        self.encoder = nn.Sequential(nn.Flatten(), nn.Linear(input_dim, latent_dim, bias=use_bias), nn.ReLU(inplace=True))\n",
        "        self.decoder = nn.Sequential(nn.Linear(latent_dim, input_dim, bias=use_bias), nn.Sigmoid())\n",
        "\n",
        "\n",
        "class DeepSymmetricAutoencoder(AbstractAutoencoder):\n",
        "    \"\"\" Standard deep AE \"\"\"\n",
        "    def __init__(self, dims: Sequence[int], use_bias=True):\n",
        "        \"\"\"\n",
        "        :param dims: seq of integers specifying the dimensions of the layers (length of dims = number of layers)\n",
        "        :param use_bias: if False, don't use bias\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert len(dims) > 0 and all(d > 0 for d in dims)\n",
        "        self.type = \"deepAE\"\n",
        "        self.use_bias = use_bias\n",
        "        enc_layers = []\n",
        "        dec_layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            enc_layers.append(nn.Linear(dims[i], dims[i + 1], bias=use_bias))\n",
        "            enc_layers.append(nn.ReLU(inplace=True))\n",
        "        for i in reversed(range(1, len(dims))):\n",
        "            dec_layers.append(nn.Linear(dims[i], dims[i - 1], bias=use_bias))\n",
        "            dec_layers.append(nn.ReLU(inplace=True))\n",
        "        dec_layers[-1] = nn.Sigmoid()\n",
        "        self.encoder = nn.Sequential(nn.Flatten(), *enc_layers)\n",
        "        self.decoder = nn.Sequential(*dec_layers)\n",
        "\n",
        "    def pretrain_layers(self, num_epochs, bs, lr, momentum, mode='basic', freeze_enc=False, **kwargs):\n",
        "        tr_data = None\n",
        "        val_data = None\n",
        "        for i, layer in enumerate(self.encoder):\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                print(f\"Pretrain layer: {layer}\")\n",
        "                # create shallow AE corresponding to the current layer\n",
        "                shallow_ae = ShallowAutoencoder(layer.in_features, layer.out_features, use_bias=self.use_bias)\n",
        "                if freeze_enc:\n",
        "                    # freeze shallow encoder's weights in case of randomized AE\n",
        "                    shallow_ae.encoder[1].weight.requires_grad = False\n",
        "                # train the shallow AE\n",
        "                shallow_ae.fit(mode=mode, tr_data=tr_data, val_data=val_data, num_epochs=num_epochs, bs=bs, lr=lr,\n",
        "                               momentum=momentum, **kwargs)\n",
        "                if freeze_enc:\n",
        "                    # in case of rand AE, copy shallow decoder's weights transpose in the shallow encoder.\n",
        "                    # This way it's possible to just copy the weights into the original model without further actions\n",
        "                    shallow_ae.encoder[1].weight = nn.Parameter(shallow_ae.decoder[0].weight.T)\n",
        "                # copy shallow AE's weights into the original bigger model\n",
        "                self.encoder[i].weight = nn.Parameter(shallow_ae.encoder[1].weight)\n",
        "                self.decoder[len(self.decoder) - i - 1].weight = nn.Parameter(shallow_ae.decoder[0].weight)\n",
        "                if self.use_bias:\n",
        "                    self.encoder[i].bias = nn.Parameter(shallow_ae.encoder[1].bias)\n",
        "                    self.decoder[len(self.decoder) - i - 1].bias = nn.Parameter(shallow_ae.decoder[0].bias)\n",
        "                # create training set for the next layer\n",
        "                if i == 1 and mode == 'denoising':  # i = 1 --> fist Linear layer\n",
        "                    tr_set, val_set = get_noisy_sets(**kwargs)\n",
        "                    tr_data, tr_targets = tr_set.data, tr_set\n",
        "                    val_data, val_targets = val_set.data, val_set.targets\n",
        "                    mode = 'basic'  # for the pretraining of the deeper layers\n",
        "                tr_data, val_data = self.create_next_layer_sets(shallow_ae=shallow_ae,\n",
        "                                                                prev_tr_data=tr_data,\n",
        "                                                                prev_val_data=val_data)\n",
        "                if num_epochs // 2 > 10:\n",
        "                    num_epochs = num_epochs // 2\n",
        "\n",
        "    @staticmethod\n",
        "    def create_next_layer_sets(shallow_ae, prev_tr_data=None, prev_val_data=None, unsqueeze=True):\n",
        "        \"\"\" Create training data for the next layer during a layer-wise pretraining \"\"\"\n",
        "        train_set, val_set = get_clean_sets()\n",
        "        prev_tr_data = train_set.data if prev_tr_data is None else prev_tr_data\n",
        "        prev_val_data = val_set.data if prev_val_data is None else prev_val_data\n",
        "        with torch.no_grad():\n",
        "            next_tr_data = torch.sigmoid(shallow_ae.encoder(prev_tr_data))\n",
        "            next_val_data = torch.sigmoid(shallow_ae.encoder(prev_val_data))\n",
        "            if unsqueeze:\n",
        "                next_tr_data, next_val_data = torch.unsqueeze(next_tr_data, 1), torch.unsqueeze(next_val_data, 1)\n",
        "        return next_tr_data, next_val_data\n",
        "\n",
        "\n",
        "class DeepSymmetricRandomizedAutoencoder(DeepSymmetricAutoencoder):\n",
        "    def __init__(self, dims: Sequence[int]):\n",
        "        super().__init__(dims=dims, use_bias=False)\n",
        "        self.type = \"deepRandAE\"\n",
        "\n",
        "    def fit(self, num_epochs=10, bs=32, lr=0.1, momentum=0., **kwargs):\n",
        "        \"\"\"\n",
        "        The training of this model is a pretraining of its layers where in the corresponding shallow AE\n",
        "        only its decoder's weights are trained, the encoder's ones are fixed.\n",
        "        Then copy the shallow decoder's weights in the corresponding layer of the bigger decoder\n",
        "        and its transpose in the corresponding layer of the bigger encoder.\n",
        "        \"\"\"\n",
        "        assert 0 < lr < 1 and num_epochs > 0 and bs > 0 and 0 <= momentum < 1\n",
        "        self.pretrain_layers(num_epochs=num_epochs, bs=bs, lr=lr, momentum=momentum, freeze_enc=True)\n",
        "\n",
        "\n",
        "class ShallowConvAutoencoder(AbstractAutoencoder):\n",
        "    \"\"\" Convolutional AE with 1 conv layer in the encoder and 1 in the decoder \"\"\"\n",
        "    def __init__(self, channels=1, n_filters=10, kernel_size: int = 3, central_dim=100,\n",
        "                 inp_side_len: Union[int, Tuple[int, int]] = 28):\n",
        "        super().__init__()\n",
        "        self.type = \"shallowConvAE\"\n",
        "        pad = (kernel_size - 1) // 2  # pad to keep the original area after convolution\n",
        "        central_side_len = math.floor(inp_side_len / 2)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=channels, out_channels=n_filters, kernel_size=kernel_size, stride=1, padding=pad),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=central_side_len ** 2 * n_filters, out_features=central_dim),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # set kernel size, padding and stride to get the correct output shape\n",
        "        kersize = 2 if central_side_len * 2 == inp_side_len else 3\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=central_dim, out_features=central_side_len ** 2 * n_filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(n_filters, central_side_len, central_side_len)),\n",
        "            nn.ConvTranspose2d(in_channels=n_filters, out_channels=channels, kernel_size=kersize, stride=2, padding=0),\n",
        "            nn.Sigmoid())\n",
        "'''\n",
        "\n",
        "class DeepConvSymmetricAutoencoder(AbstractAutoencoder):\n",
        "    \"\"\" Conv AE with variable number of conv layers \"\"\"\n",
        "    def __init__(self, inp_side_len=32, dims=(32, 64), kernel_sizes=3, central_dim=256, pool=True, in_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # initial checks\n",
        "        if isinstance(kernel_sizes, int):\n",
        "            kernel_sizes = [kernel_sizes] * len(dims)\n",
        "        assert len(kernel_sizes) == len(dims) and all(size > 0 for size in kernel_sizes)\n",
        "\n",
        "        # build encoder\n",
        "        step_pool = 1 if len(dims) < 3 else (2 if len(dims) < 6 else 3)\n",
        "        side_len = inp_side_len\n",
        "        side_lengths = [side_len]\n",
        "        dims = (in_channels, *dims)  # Set the first dimension to the number of input channels (e.g., 3 for RGB)\n",
        "        enc_layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            pad = (kernel_sizes[i] - 1) // 2\n",
        "            enc_layers.append(nn.Conv2d(in_channels=dims[i], out_channels=dims[i + 1], kernel_size=kernel_sizes[i],\n",
        "                                        padding=pad, stride=1))\n",
        "            enc_layers.append(nn.ReLU(inplace=True))\n",
        "            if pool and (i % step_pool == 0 or i == len(dims) - 1) and side_len > 3:\n",
        "                enc_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "                side_len = math.floor(side_len / 2)\n",
        "                side_lengths.append(side_len)\n",
        "\n",
        "        # fully connected layers in the center of the autoencoder to reduce dimensionality\n",
        "        fc_dims = (side_len ** 2 * dims[-1], side_len ** 2 * dims[-1] // 2, central_dim)\n",
        "        self.encoder = nn.Sequential(\n",
        "            *enc_layers,\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(fc_dims[0], fc_dims[1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(fc_dims[1], fc_dims[2]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # build decoder\n",
        "        central_side_len = side_lengths.pop(-1)\n",
        "        dec_layers = []\n",
        "        for i in reversed(range(1, len(dims))):\n",
        "            kersize = 2 if len(side_lengths) > 0 and side_len * 2 == side_lengths.pop(-1) else 3\n",
        "            pad, stride = (1, 1) if side_len == inp_side_len else (0, 2)\n",
        "            dec_layers.append(nn.ConvTranspose2d(in_channels=dims[i], out_channels=dims[i - 1], kernel_size=kersize,\n",
        "                                                 padding=pad, stride=stride))\n",
        "            side_len = side_len if pad == 1 else (side_len * 2 if kersize == 2 else side_len * 2 + 1)\n",
        "            dec_layers.append(nn.ReLU(inplace=True))\n",
        "        dec_layers[-1] = nn.Sigmoid()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(fc_dims[2], fc_dims[1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(fc_dims[1], fc_dims[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(dims[-1], central_side_len, central_side_len)),\n",
        "            *dec_layers,\n",
        "        )"
      ],
      "metadata": {
        "id": "f-aB4wISp4xv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My code starts from here"
      ],
      "metadata": {
        "id": "CCt9pRSRqpa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, save_dir, num_epochs):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint from a specified directory into the model.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module): The model to load the checkpoint into.\n",
        "        save_dir (str): The directory containing checkpoint files.\n",
        "        num_epochs (int): The total number of epochs to train.\n",
        "\n",
        "    Returns:\n",
        "        int: The epoch number to start training from.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Check for checkpoint files in the specified directory\n",
        "    checkpoint_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]\n",
        "    training_finished = False\n",
        "\n",
        "    if checkpoint_files:\n",
        "        if \"model_weights.pth\" in checkpoint_files:\n",
        "            latest_checkpoint_file = \"model_weights.pth\"\n",
        "            training_finished = True\n",
        "        else:\n",
        "            # Find the checkpoint file with the highest epoch number\n",
        "            latest_checkpoint_file = max(checkpoint_files, key=lambda x: int(re.search(r'(\\d+)', x).group()))\n",
        "\n",
        "        # Load the state_dict from the checkpoint file into the model\n",
        "        model.load_state_dict(torch.load(os.path.join(save_dir, latest_checkpoint_file)))\n",
        "\n",
        "        # Determine the starting epoch\n",
        "        start_epoch = num_epochs if training_finished else int(re.search(r'(\\d+)', latest_checkpoint_file).group()) + 1\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "FS7K3Su6q1gH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, start_epoch, num_epochs, save_dir, train_loader, val_loader, criterion, optimizer, device, accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    Trains a model and saves checkpoints.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        start_epoch (int): The starting epoch for resuming training.\n",
        "        num_epochs (int): Total number of epochs to train.\n",
        "        save_dir (str): Directory to save checkpoints.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        val_loader (DataLoader): DataLoader for validation data.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimizer for model training.\n",
        "        device: Device to perform computations on (e.g., 'cuda' or 'cpu').\n",
        "        accumulation_steps (int): Number of steps to accumulate gradients before updating.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Starting training from epoch {start_epoch + 1}\")\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        accumulated_loss = 0.0\n",
        "\n",
        "        # Training phase\n",
        "        for i, (inputs, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = inputs  # For autoencoders, the target is the input itself\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets) / accumulation_steps  # Scale the loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            accumulated_loss += loss.item()\n",
        "\n",
        "            # Update the model and reset gradients after accumulation\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print statistics every 100 mini-batches\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "        # Validation phase - every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, _ in val_loader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    targets = inputs\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f\"Validation Loss after epoch {epoch + 1}: {val_loss:.4f}\")\n",
        "\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "QBOKBEDPrnsZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_reconstruction(model, val_loader, device, n=1):\n",
        "    \"\"\"\n",
        "    Displays original and reconstructed images from a model.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module): The autoencoder model.\n",
        "        val_loader (DataLoader): Validation data loader.\n",
        "        device: Device to perform computations on (e.g., 'cuda' or 'cpu').\n",
        "        n (int): Number of images to display.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Obtain one batch of validation data\n",
        "    val_iter = iter(val_loader)\n",
        "    images, _ = next(val_iter)\n",
        "\n",
        "    # Move images to the device\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Pass through the autoencoder to get reconstructed images\n",
        "    with torch.no_grad():\n",
        "        reconstructed_images = model(images)\n",
        "\n",
        "    # Move images back to CPU for visualization\n",
        "    images = images.cpu()\n",
        "    reconstructed_images = reconstructed_images.cpu()\n",
        "\n",
        "    # Display original and reconstructed images\n",
        "    fig, axes = plt.subplots(n, 2, figsize=(6, 3 * n))\n",
        "    for i in range(n):\n",
        "        if n == 1:\n",
        "            axes[0].imshow(transforms.ToPILImage()(images[i]))\n",
        "            axes[0].set_title(\"Original\")\n",
        "            axes[0].axis('off')\n",
        "            axes[1].imshow(transforms.ToPILImage()(reconstructed_images[i]))\n",
        "            axes[1].set_title(\"Reconstructed\")\n",
        "            axes[1].axis('off')\n",
        "        else:\n",
        "            axes[i, 0].imshow(transforms.ToPILImage()(images[i]))\n",
        "            axes[i, 0].set_title(\"Original\")\n",
        "            axes[i, 0].axis('off')\n",
        "            axes[i, 1].imshow(transforms.ToPILImage()(reconstructed_images[i]))\n",
        "            axes[i, 1].set_title(\"Reconstructed\")\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Feb6vNKksCvP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data transformations for CIFAR-100\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # You can add normalization if needed\n",
        "    # transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "val_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWO6LvamXx6f",
        "outputId": "e25af830-4e0c-45b0-85c5-fcef91a6dfbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 169M/169M [00:02<00:00, 77.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "num_epochs = 250"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHZEI29svXx0",
        "outputId": "b4480167-7cc1-4190-daa2-ecb0c4498c44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gk5hs9xywtBu"
      },
      "outputs": [],
      "source": [
        "# Instantiate your model\n",
        "model = DeepConvSymmetricAutoencoder(\n",
        "    inp_side_len=32,  # CIFAR-100 images are 32x32\n",
        "    dims=(32, 64, 128, 256, 512),    # 5 layers\n",
        "    kernel_sizes=3,\n",
        "    central_dim=256,\n",
        "    in_channels=3  # CIFAR-100 has 3 channels (RGB)\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IWoWi8bK80j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ff415f-77b5-474a-fe37-d057d0cf3067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c97416bc3fbe>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(save_dir, latest_checkpoint_file)))\n"
          ]
        }
      ],
      "source": [
        "save_dir_symmetric_ae = '/content/gdrive/My Drive/checkpoints/SymmetricAutoencoder'\n",
        "start_epoch_symmetric = load_checkpoint(model, save_dir_symmetric_ae, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to solve the model's crashing for the 10 dims (out of memory)\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "Q44JY-nA0_Rg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, start_epoch_symmetric, num_epochs, save_dir_symmetric_ae, train_loader, val_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "ELgnq1ZtYisB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240755ef-83ae-4d93-da2c-4898b0882ff6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training from epoch 252\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_reconstruction(model, val_loader, device, n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "s3q5U3hIJogw",
        "outputId": "720ce896-2efd-472b-fc15-1afa185d5863"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVSAH_e6tvmt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asymmetric Autoencoders"
      ],
      "metadata": {
        "id": "OE-VLUmCuIU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My addition - asymmetric autoencoder (currently adjusted for smaller decoder only)\n",
        "\n",
        "def remove_dims(dims, num_layers_less, smaller_part='decoder'):\n",
        "    if num_layers_less >= len(dims):\n",
        "        # If num_layers_less is too large, retain only one dimension\n",
        "        return [dims[0]] if smaller_part == 'encoder' else [dims[-1]]\n",
        "\n",
        "    # if num_layers_less == 0:\n",
        "        # return dims\n",
        "    adjusted_dims = []\n",
        "\n",
        "    times_skipped = 0\n",
        "    if smaller_part == 'encoder':\n",
        "        # Start with the first dimension included\n",
        "        adjusted_dims = [dims[0]]\n",
        "        skip = True  # Start by skipping the second element\n",
        "\n",
        "        # Iterate through remaining dims from start (excluding the first element)\n",
        "        for dim in dims[1:]:\n",
        "          if times_skipped >= num_layers_less:\n",
        "                skip = False\n",
        "          times_skipped = times_skipped + 1\n",
        "          if not skip:\n",
        "              adjusted_dims.append(dim)\n",
        "              times_skipped = times_skipped - 1\n",
        "          skip = not skip  # Toggle skipping\n",
        "\n",
        "\n",
        "    else:  # smaller_part == 'decoder'\n",
        "        # Start with the last dimension included\n",
        "        adjusted_dims = [dims[-1]]\n",
        "        skip = True  # Start by skipping the second-last layer\n",
        "\n",
        "        # Iterate through remaining dims in reverse, excluding the last element\n",
        "        for dim in reversed(dims[:-1]):\n",
        "          if times_skipped >= num_layers_less:\n",
        "              skip = False\n",
        "          times_skipped = times_skipped + 1\n",
        "          if not skip:\n",
        "              adjusted_dims.append(dim)\n",
        "              times_skipped = times_skipped - 1\n",
        "          skip = not skip  # Toggle skipping\n",
        "\n",
        "\n",
        "        # Reverse back to maintain the original order\n",
        "        adjusted_dims.reverse()\n",
        "\n",
        "    return tuple(adjusted_dims)\n",
        "\n",
        "\n",
        "# Function to process kernel sizes for encoder and decoder separately\n",
        "def process_kernel_sizes(kernel_sizes, adjusted_dims_enc, adjusted_dims_dec):\n",
        "    if isinstance(kernel_sizes, int):\n",
        "        # Apply the same kernel size to all encoder and decoder layers\n",
        "        kernel_sizes_enc = [kernel_sizes] * len(adjusted_dims_enc)\n",
        "        kernel_sizes_dec = [kernel_sizes] * len(adjusted_dims_dec)\n",
        "    else:\n",
        "        # Ensure kernel_sizes is long enough for both encoder and decoder, should never get here currently\n",
        "        assert len(kernel_sizes) == len(adjusted_dims_enc) + len(adjusted_dims_dec) - 2, \\\n",
        "        \"kernel_sizes should have length equal to (encoder layers + decoder layers - 2)\"\n",
        "\n",
        "        # Split kernel_sizes between encoder and decoder\n",
        "        kernel_sizes_enc = kernel_sizes[:len(adjusted_dims_enc) - 1]\n",
        "        kernel_sizes_dec = kernel_sizes[len(adjusted_dims_enc) - 1:]\n",
        "\n",
        "    # Verify each kernel size is positive\n",
        "    assert all(size > 0 for size in kernel_sizes_enc + kernel_sizes_dec), \\\n",
        "        \"All kernel sizes must be positive integers\"\n",
        "\n",
        "    return kernel_sizes_enc, kernel_sizes_dec\n",
        "\n",
        "class DeepConvAsymmetricAutoencoder(AbstractAutoencoder):\n",
        "    \"\"\" Conv AE with variable number of conv layers, where either encoder or decoder has fewer layers \"\"\"\n",
        "    def __init__(self, inp_side_len=32, dims=(32, 64), kernel_sizes=3, central_dim=256, pool=True, in_channels=3,\n",
        "                 smaller_part='decoder', num_layers_less=1):\n",
        "        \"\"\"\n",
        "        :param inp_side_len: Input image side length (e.g., 32 for CIFAR)\n",
        "        :param dims: Dimensions of convolutional layers (e.g., (32, 64))\n",
        "        :param kernel_sizes: Kernel sizes for each layer\n",
        "        :param central_dim: Central bottleneck dimension\n",
        "        :param pool: Whether to use pooling\n",
        "        :param in_channels: Number of input channels (3 for RGB)\n",
        "        :param smaller_part: 'encoder' or 'decoder', indicating which part should have fewer layers\n",
        "        :param num_layers_less: How many fewer layers the smaller part should have\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert smaller_part in ['encoder', 'decoder'], \"smaller_part must be 'encoder' or 'decoder'\"\n",
        "        assert num_layers_less >= 0, \"num_layers_less must be non-negative\"\n",
        "\n",
        "        # Adjust dims based on which part is smaller\n",
        "        if smaller_part == 'encoder':\n",
        "            adjusted_dims_enc = remove_dims(dims, num_layers_less, smaller_part)\n",
        "            adjusted_dims_dec = dims\n",
        "        else:\n",
        "            adjusted_dims_enc = dims\n",
        "            adjusted_dims_dec = remove_dims(dims, num_layers_less, smaller_part)\n",
        "\n",
        "        kernel_sizes_enc, kernel_sizes_dec = process_kernel_sizes(kernel_sizes, adjusted_dims_enc, adjusted_dims_dec)\n",
        "\n",
        "        # Encoder construction\n",
        "        step_pool = 1 if len(adjusted_dims_enc) < 3 else (2 if len(adjusted_dims_enc) < 6 else 3)\n",
        "        side_len = inp_side_len\n",
        "        side_lengths = [side_len]\n",
        "\n",
        "        adjusted_dims_enc = (in_channels, *adjusted_dims_enc)  # Set the first dimension to input channels\n",
        "        adjusted_dims_dec = (in_channels, *adjusted_dims_dec)  # Ensure the last dimension matches input channels\n",
        "\n",
        "        enc_layers = []\n",
        "        for i in range(len(adjusted_dims_enc) - 1):\n",
        "            pad = (kernel_sizes_enc[i] - 1) // 2\n",
        "            enc_layers.append(nn.Conv2d(in_channels=adjusted_dims_enc[i], out_channels=adjusted_dims_enc[i + 1],\n",
        "                                        kernel_size=kernel_sizes_enc[i], padding=pad, stride=1))\n",
        "            enc_layers.append(nn.ReLU(inplace=True))\n",
        "            if pool and (i % step_pool == 0 or i == len(adjusted_dims_enc) - 1) and side_len > 3:\n",
        "                enc_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "                side_len = math.floor(side_len / 2)\n",
        "                side_lengths.append(side_len)\n",
        "\n",
        "        fc_dims_enc = (side_len ** 2 * adjusted_dims_enc[-1], side_len ** 2 * adjusted_dims_enc[-1] // 2, central_dim)\n",
        "        fc_dims_dec = (side_len ** 2 * adjusted_dims_dec[-1], side_len ** 2 * adjusted_dims_dec[-1] // 2, central_dim)\n",
        "        self.encoder = nn.Sequential(\n",
        "            *enc_layers,\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(fc_dims_enc[0], fc_dims_enc[1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(fc_dims_enc[1], fc_dims_enc[2]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder construction\n",
        "        central_side_len = side_lengths.pop(-1)\n",
        "        dec_layers = []\n",
        "        for i in reversed(range(1, len(adjusted_dims_dec))):\n",
        "            kersize = 2 if len(side_lengths) > 0 and side_len * 2 == side_lengths.pop(-1) else 3\n",
        "            pad, stride = (1, 1) if side_len == inp_side_len else (0, 2)\n",
        "            dec_layers.append(nn.ConvTranspose2d(in_channels=adjusted_dims_dec[i], out_channels=adjusted_dims_dec[i - 1], kernel_size=kersize,\n",
        "                                                padding=pad, stride=stride))\n",
        "            side_len = side_len if pad == 1 else (side_len * 2 if kersize == 2 else side_len * 2 + 1)\n",
        "            dec_layers.append(nn.ReLU(inplace=True))\n",
        "        dec_layers[-1] = nn.Sigmoid()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(fc_dims_dec[2], fc_dims_dec[1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(fc_dims_dec[1], fc_dims_dec[0]),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(dims[-1], central_side_len, central_side_len)),\n",
        "            *dec_layers,\n",
        "        )"
      ],
      "metadata": {
        "id": "oceGVqER4CAm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to solve the model's crashing for the 10 dims (out of memory)\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "s8h8q1a1A8gn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S0jemCIIAKT_"
      },
      "outputs": [],
      "source": [
        "# Instantiate your model\n",
        "asym_model_minus2 = DeepConvAsymmetricAutoencoder(\n",
        "    inp_side_len=32,  # CIFAR-100 images are 32x32\n",
        "    dims=(32, 64, 128, 256, 512),    # 5 layers\n",
        "    kernel_sizes=3,\n",
        "    central_dim=256,\n",
        "    num_layers_less=2,\n",
        "    in_channels=3  # CIFAR-100 has 3 channels (RGB)\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(asym_model_minus2.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P49s6qHI3Ent",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b3d2e7-159f-4ff0-8f68-24e51f17bb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c97416bc3fbe>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(save_dir, latest_checkpoint_file)))\n"
          ]
        }
      ],
      "source": [
        "save_dir_asymmetric_ae_minus2 = '/content/gdrive/My Drive/checkpoints/AsymmetricAutoencoderMinus2'\n",
        "start_epoch_asym_minus2 = load_checkpoint(asym_model_minus2, save_dir_asymmetric_ae_minus2, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xGQhq3lrcQen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26da422-6aa9-4334-ccf3-8ed387640b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training from epoch 252\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train_model(asym_model_minus2, start_epoch_asym_minus2, num_epochs, save_dir_asymmetric_ae_minus2, train_loader, val_loader, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2R6yx6T4ndc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_reconstruction(asym_model_minus2, val_loader, device, n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "qWomjan7KMsY",
        "outputId": "b6f9fa0c-0664-4cdc-824a-f3985353ede8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eZXwyV_W3yfQ"
      },
      "outputs": [],
      "source": [
        "# Instantiate your model\n",
        "asym_model_minus1 = DeepConvAsymmetricAutoencoder(\n",
        "    inp_side_len=32,  # CIFAR-100 images are 32x32\n",
        "    dims=(32, 64, 128, 256, 512),    # 5 layers\n",
        "    kernel_sizes=3,\n",
        "    central_dim=256,\n",
        "    in_channels=3  # CIFAR-100 has 3 channels (RGB)\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(asym_model_minus1.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oq8xNpqOJtKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58eb14e-4ab6-4cd5-8db8-ea7690c9a325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c97416bc3fbe>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(save_dir, latest_checkpoint_file)))\n"
          ]
        }
      ],
      "source": [
        "save_dir_asymmetric_ae_minus1 = '/content/gdrive/My Drive/checkpoints/AsymmetricAutoencoderMinus1'\n",
        "start_epoch_asym_minus1 = load_checkpoint(asym_model_minus1, save_dir_asymmetric_ae_minus1, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ONnfEf_M3yWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181e8b0d-1b29-4fee-b65a-48a847724b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training from epoch 252\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train_model(asym_model_minus1, start_epoch_asym_minus1, num_epochs, save_dir_asymmetric_ae_minus1, train_loader, val_loader, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KV4Occ9zhAQA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_reconstruction(asym_model_minus1, val_loader, device, n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "WSd_PeK9KV8U",
        "outputId": "d06462e9-1c12-4c0d-db46-b5aaea06ffb2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "z0PsJsgrsBI-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "sym_model_by_asym_code = DeepConvAsymmetricAutoencoder(\n",
        "    inp_side_len=32,  # CIFAR-100 images are 32x32\n",
        "    dims=(32, 64, 128, 256, 512),    # 5 layers\n",
        "    kernel_sizes=3,\n",
        "    central_dim=256,\n",
        "    num_layers_less=0,\n",
        "    in_channels=3  # CIFAR-100 has 3 channels (RGB)\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(sym_model_by_asym_code.parameters(), lr=0.001)\n",
        "\n",
        "save_dir_symmetric_by_asymmetric_code = '/content/gdrive/My Drive/checkpoints/SymmetricAutoencoderByAsymmetricCode'\n",
        "start_epoch_symmetric_by_asym_code = load_checkpoint(sym_model_by_asym_code, save_dir_symmetric_by_asymmetric_code, num_epochs)\n",
        "\n",
        "train_model(\n",
        "    sym_model_by_asym_code,\n",
        "    start_epoch_symmetric_by_asym_code,\n",
        "    num_epochs,\n",
        "    save_dir_symmetric_by_asymmetric_code,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device\n",
        ")\n",
        "\n",
        "display_reconstruction(sym_model_by_asym_code, val_loader, device, n=1)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "JUMTRw9TcBzr",
        "outputId": "0acc75e4-4ecf-4d4e-940c-553b3d5fedeb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nsym_model_by_asym_code = DeepConvAsymmetricAutoencoder(\\n    inp_side_len=32,  # CIFAR-100 images are 32x32\\n    dims=(32, 64, 128, 256, 512),    # 5 layers\\n    kernel_sizes=3,\\n    central_dim=256,\\n    num_layers_less=0,\\n    in_channels=3  # CIFAR-100 has 3 channels (RGB)\\n).to(device)\\n\\n\\n# Define loss function and optimizer\\ncriterion = nn.MSELoss()\\noptimizer = optim.Adam(sym_model_by_asym_code.parameters(), lr=0.001)\\n\\nsave_dir_symmetric_by_asymmetric_code = '/content/gdrive/My Drive/checkpoints/SymmetricAutoencoderByAsymmetricCode'\\nstart_epoch_symmetric_by_asym_code = load_checkpoint(sym_model_by_asym_code, save_dir_symmetric_by_asymmetric_code, num_epochs)\\n\\ntrain_model(\\n    sym_model_by_asym_code,\\n    start_epoch_symmetric_by_asym_code,\\n    num_epochs,\\n    save_dir_symmetric_by_asymmetric_code,\\n    train_loader,\\n    val_loader,\\n    criterion,\\n    optimizer,\\n    device\\n)\\n\\ndisplay_reconstruction(sym_model_by_asym_code, val_loader, device, n=1)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MSvXMRGkgfoN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mlKN5sDwdv50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sCG9df5AQYlK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b4EkyZidGpqa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2JdUt-urFv9K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1Edj_W-i3yCf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4LMbpZ0pzsCr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FzOvO6-WCVi7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}